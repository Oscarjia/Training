{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2019年9月28日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "回答以下问题，并将问题发送至 minchuian.gao@gmail.com 中：\n",
    "```\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    2.2. what problems do you want to solve？\n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    2.5. How will you plan to study in this course period?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至minchuian.gao@gmail.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.10.8日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "1) Use AI to select good quality seed for Agriculture <br>\n",
    "2) Qualitative trade using AI <br>\n",
    "3) Use computer vision to recover old movies/ videos <br>\n",
    "4) Use NLP robot to generate ancient poetry （e.g. AI 李白）<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <br>\n",
    "1) Github is good for code version control and easy code sharing and communication. <br>\n",
    "2) Jupyter noetbook is good for running code line by line, incorporate code, output, data frame, data visulization in one place. But has the disadvantages that same lots of temporary variables in local environment. <br>\n",
    "3) Pycharm is good for build more complicated code, python module or data pipeline.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Probability model in this class refers to use probability to predict the chance of a sentences might occur based on a provided vocabulary/ text pool.  It calculates the probability of words appear in a specific sequence based on the vocabulary/ text pool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "1) Identify the correct audio input from Siri or Amazon echo <br>\n",
    "2) Grammar or spelling suggestion software (e.g Grammarly) <br>\n",
    "3) Machine Translation (e.g.  Chinese to English ) <br>\n",
    "4) Detect spam emails (Naïve Bayeys models, but it is modeled as an unordered collection of words selected from one of two probability distributions )<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Parsing and pattern match are not flexible enough and is difficult to be generalized to new cases.  Probability models are not trying to predict the exact right answer based on existing  fixed rules or syntax, but to choose the most likely answer based on probability.  Therefore, it’s more easy for code management and generalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Language Model in NLP means a statistical language model that use a probability distribution over sequences of words to calcualte the probablity of the occurrence of sequence. It can provide context to distinguish between words and phrases that sound similar.  Unigram or 2 gram model we discussed in the first lecture are examples of language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Machine Translation, Text summarization,  Machine dialog system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:1-gram language model is also called unigram model or bag of words model. It calculated the probability of one sentence based on the apperance of word in the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: It is an orderless document representation — only the counts of words matters. \n",
    "This method is used for document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 2 gram model use a two words window to cut sentence and calculated the probablity of a sentence given 1 previous word. N-gram模型的优点在于它包含了前N-1个词所能提供的全部信息，这些词对于当前词的出现具有很强的约束力，然而它的缺点是需要相当规模的训练文本来确定模型的参数。当N很大时，模型的参数空间过大。所以常见的N值一般为1,2。还有因数据稀疏而导致的数据平滑问题，解决方法主要是使所有的N-gram概率之和为1和使所有的N-gram概率都不为0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个英语语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_robot = '''\n",
    "ask_question= guest_name | question_start | product | question_end\n",
    "guest_name = you | Sir | lady | mandam | Miss | little sweet heart | gentle man \n",
    "question_start = Can I ask | What question do | tell me | Which \n",
    "product = which model | product | iPhone | iPad | Macbook | watch | earphone \n",
    "question_end = interested? | want to buy? | take a look? | have question about? \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个英语语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest = '''\n",
    "answer_question = guest_name | action | product | model\n",
    "guest_name = I | My mom | My dad | My friend | My daughter | My son \n",
    "action = am(is) looking for | want to see | want to try | take a look | want to test\n",
    "product = iPhone | Ipad | Macbook | watch | earphone \n",
    "model = 11 Pro | 11 | XsMax | Xs| XR | X |8 | 7 \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三个中文语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = '''\n",
    "description= owner| adv | verb | product | adj\n",
    "owner = 我 | 你 | 他 | 她 | 我朋友 | 我妈妈 | 我爸爸\n",
    "adv = 特别 | 一点也不 | 真得 | 很 | 太 \n",
    "verb = 觉得 | 认为 | 想 | 喜欢 | 讨厌 \n",
    "product = 这个火锅 | 这个手机 | 这部电影 | 这个餐馆 | 这件衣服 \n",
    "adj = 好吃| 好用 | 好看 | 美味 | 显瘦 | 精彩 | \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四个中文语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " insurance= '''\n",
    "promotion = guest_name | product | verb | adv\n",
    "guest_name = 先生 | 女士 | 你 | 王老板  \n",
    "product = 这款保险 | 这个产品| 本款保单 | 这个项目 \n",
    "verb = 真的 | 卖地 | 购买 |  投资\n",
    "adv = 很好 | 收益高 | 人多 | 不错 | 保险 |风险低 | 回报稳定\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。\n",
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我的三个函数：\n",
    "英文句子词与词之间需要空格，所以我现在的function 还没有很好的对这一点进行处理，三个函数主要针对中文语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def standardize_to_dict(grammar_rule, stmt_split='=', or_split='|'):\n",
    "    #print ('raw grammer rule: ', grammar_rule)\n",
    "    standardized_rule = dict() # key is the @statement, value is @expression\n",
    "    for line in grammar_rule.split('\\n'):\n",
    "        if not line: continue #skip the empty line \n",
    "        stmt, expr = line.split(stmt_split)\n",
    "        #English no need to remove space\n",
    "        #standardized_rule[stmt.strip().replace(' ','')] = expr.split(or_split)\n",
    "        #中文需要去掉space \n",
    "        standardized_rule[stmt.strip()] = expr.replace(' ','').split(or_split)\n",
    "    #print ('standardized_rule: ' , standardized_rule)\n",
    "    return standardized_rule\n",
    "\n",
    "def generate_one_sentence(standardized_rule, target, language):\n",
    "    sentence=[]\n",
    "    if target in standardized_rule.keys():\n",
    "        for i in standardized_rule[target]:\n",
    "            sentence.append((random.choice(standardized_rule[i])))  \n",
    "        #print ('generated sentence: ', ''.join(x for x in generated))\n",
    "        #for english we want to keep one space between words: return ' '.join(x for x in sentence)\n",
    "        if language=='Chinese':\n",
    "            return ''.join(x for x in sentence)\n",
    "        else:\n",
    "            return ' '.join(x for x in sentence)\n",
    "    else:\n",
    "        print ('No target nameed ', target, ' in grammer_rules')\n",
    "        return None\n",
    "\n",
    "def generate_n_sentence(n):\n",
    "    sentences=[]\n",
    "    for i in range(n):\n",
    "        print (generate_one_sentence(standardized_rule, target,language))\n",
    "        sentences.append(generate_one_sentence(standardized_rule, target,language))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用英文语法1产生10个句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss tellme product wanttobuy?\n",
      "\n",
      "\n",
      "littlesweetheart Whatquestiondo watch havequestionabout?\n",
      "littlesweetheart CanIask Macbook havequestionabout?\n",
      "Sir Whatquestiondo iPad havequestionabout?\n",
      "you CanIask iPad interested?\n",
      "gentleman Which earphone havequestionabout?\n",
      "Sir CanIask Macbook wanttobuy?\n",
      "mandam tellme iPad wanttobuy?\n",
      "littlesweetheart Whatquestiondo watch takealook?\n",
      "gentleman Which earphone wanttobuy?\n",
      "gentleman Whatquestiondo iPhone havequestionabout?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar_rule=sales_robot\n",
    "target='ask_question'\n",
    "language='English'\n",
    "standardized_rule=standardize_to_dict(grammar_rule, stmt_split='=', or_split='|')\n",
    "sentence=generate_one_sentence(standardized_rule, target,language) \n",
    "print (sentence)\n",
    "print ('\\n')\n",
    "y1_sentence10=generate_n_sentence(10)\n",
    "print ('\\n')\n",
    "#print (y1_sentence10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用英文语法2产生10个句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I wanttosee earphone 11Pro\n",
      "\n",
      "\n",
      "Myfriend wanttotest earphone 7\n",
      "Myson takealook earphone 11Pro\n",
      "Myfriend wanttotest Macbook 11Pro\n",
      "Mydad wanttotest watch XsMax\n",
      "Myson am(is)lookingfor earphone 8\n",
      "I takealook Ipad X\n",
      "Mydad wanttotest watch X\n",
      "Myson wanttotry watch 7\n",
      "I wanttotry Ipad 7\n",
      "Myson wanttosee earphone XR\n"
     ]
    }
   ],
   "source": [
    "grammar_rule=guest\n",
    "target='answer_question'\n",
    "language='English'\n",
    "standardized_rule=standardize_to_dict(grammar_rule, stmt_split='=', or_split='|')\n",
    "sentence=generate_one_sentence(standardized_rule, target, language) \n",
    "print (sentence)\n",
    "print ('\\n')\n",
    "y2_sentence10=generate_n_sentence(10)\n",
    "#print (y2_sentence10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用中文语法三产生20个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他真得认为这个火锅好用\n",
      "\n",
      "\n",
      "我特别讨厌这个餐馆\n",
      "我爸爸特别想这件衣服精彩\n",
      "我爸爸太喜欢这个手机显瘦\n",
      "我朋友真得觉得这个餐馆好吃\n",
      "我爸爸真得觉得这个餐馆好吃\n",
      "他太认为这个手机美味\n",
      "他真得认为这部电影精彩\n",
      "我爸爸真得讨厌这件衣服美味\n",
      "我朋友真得喜欢这部电影美味\n",
      "我朋友很想这个餐馆好看\n",
      "他真得讨厌这个火锅美味\n",
      "我朋友太讨厌这部电影好看\n",
      "他真得喜欢这件衣服精彩\n",
      "我妈妈真得喜欢这件衣服好吃\n",
      "他很想这个火锅好用\n",
      "我妈妈真得喜欢这部电影\n",
      "我特别想这部电影好看\n",
      "我妈妈一点也不讨厌这件衣服\n",
      "我爸爸一点也不认为这个餐馆好用\n",
      "我很觉得这个餐馆\n"
     ]
    }
   ],
   "source": [
    "grammar_rule=review\n",
    "target='description'\n",
    "language='Chinese'\n",
    "standardized_rule=standardize_to_dict(grammar_rule, stmt_split='=', or_split='|')\n",
    "sentence=generate_one_sentence(standardized_rule, target, language) \n",
    "print (sentence)\n",
    "print ('\\n')\n",
    "c1_sentences=generate_n_sentence(20)\n",
    "#print (c1_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用中文语法四产生20个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你本款保单投资很好\n",
      "\n",
      "\n",
      "王老板这个项目购买人多\n",
      "王老板这款保险购买回报稳定\n",
      "先生这个项目购买风险低\n",
      "王老板这款保险投资很好\n",
      "王老板本款保单真的不错\n",
      "你这款保险卖地保险\n",
      "王老板这个产品真的风险低\n",
      "女士本款保单卖地很好\n",
      "女士这个产品真的保险\n",
      "你这款保险真的很好\n",
      "你本款保单投资收益高\n",
      "女士这个项目真的回报稳定\n",
      "王老板本款保单投资很好\n",
      "女士这款保险购买保险\n",
      "先生这个产品真的收益高\n",
      "女士这款保险真的很好\n",
      "你这款保险真的风险低\n",
      "你本款保单投资回报稳定\n",
      "王老板这个产品卖地人多\n",
      "先生这个产品真的人多\n"
     ]
    }
   ],
   "source": [
    "grammar_rule=insurance\n",
    "target='promotion'\n",
    "language='Chinese'\n",
    "standardized_rule=standardize_to_dict(grammar_rule, stmt_split='=', or_split='|')\n",
    "sentence=generate_one_sentence(standardized_rule, target, language) \n",
    "print (sentence)\n",
    "print ('\\n')\n",
    "c2_sentences=generate_n_sentence(20)\n",
    "#print (c2_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(sentence) = Pr(w_1 \\cdot w_2 \\cdots w_n) = \\prod \\frac{count(w_i, w_{i+1})}{count(w_{i+1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,link,name,comment,star\r\n",
      "1,https://movie.douban.com/subject/26363254/,战狼2,吴京意淫到了脑残的地步，看了恶心想吐,1\r\n",
      "2,https://movie.douban.com/subject/26363254/,战狼2,首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹,2\r\n"
     ]
    }
   ],
   "source": [
    "! head -3 movie_comments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ++$++ disability-insurance ++$++ 法律要求残疾保险吗？ ++$++ Is  Disability  Insurance  Required  By  Law?\r\n",
      "1 ++$++ life-insurance ++$++ 债权人可以在死后人寿保险吗？ ++$++ Can  Creditors  Take  Life  Insurance  After  Death?\r\n",
      "2 ++$++ renters-insurance ++$++ 旅行者保险有租赁保险吗？ ++$++ Does  Travelers  Insurance  Have  Renters  Insurance?\r\n"
     ]
    }
   ],
   "source": [
    "! head -3 train_insurance.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "def get_chinese_only_from_train(train_file):\n",
    "    train = train_file\n",
    "    train = open(train,encoding=\"utf8\", errors='ignore').read().lower()\n",
    "    #train = open(train).read().lower()\n",
    "    chinese_only=re.findall(r'([\\u4e00-\\u9fff]+)',train)\n",
    "    print ('First 20 Chinese characters of the train file is :', ''.join(chinese_only)[:20])\n",
    "    return ''.join(chinese_only)\n",
    "\n",
    "def process_train(clean_train):\n",
    "    from collections import Counter\n",
    "    #read train text\n",
    "    train = clean_train\n",
    "    print ('Length of cleaned train file: ', len(train))\n",
    "    TOKENS = list(jieba.cut(train))\n",
    "    print ('Train file contains ', len(TOKENS), ' of words(tokens).')\n",
    "    _1_gram_word_counts = Counter(TOKENS)\n",
    "    _2_gram_words = [ TOKENS[i] + TOKENS[i+1] for i in range(len(TOKENS)-1) ]\n",
    "    #get frequence of 2 gram word from train \n",
    "    _2_gram_word_counts = Counter(_2_gram_words)\n",
    "    return _1_gram_word_counts, _2_gram_word_counts\n",
    "\n",
    "\n",
    "def get_gram_count(word, n_gram_word_counts):\n",
    "    if word in n_gram_word_counts: return n_gram_word_counts[word]\n",
    "    else:\n",
    "        return n_gram_word_counts.most_common()[-1][-1]\n",
    "\n",
    "#calculate the probability of the sentence based on 2 gram model:\n",
    "def two_gram_model(sentence):\n",
    "    # 2-gram langauge model\n",
    "    tokens = list(jieba.cut(sentence))\n",
    "    probability = 1\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        #look up 2_gram count for words in sentence\n",
    "        _two_gram_c = get_gram_count(word+next_word, _2_gram_word_counts)\n",
    "        #look up 1_gram count for words in sentence\n",
    "        _one_gram_c = get_gram_count(next_word, _1_gram_word_counts)\n",
    "        pro =  _two_gram_c / _one_gram_c\n",
    "        probability *= pro \n",
    "    return probability  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use movie_review as train dataset，calculate the probability of the first sentence in c1_sentences using 2 gram model："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 Chinese characters of the train file is : 战狼吴京意淫到了脑残的地步看了恶心想吐战\n",
      "Length of cleaned train file:  8891829\n",
      "Train file contains  5112467  of words(tokens).\n",
      "probability of sentence:  我一点也不认为这个手机美味 is  4.118373402282527e-15\n"
     ]
    }
   ],
   "source": [
    "train_file='./movie_comments.csv'\n",
    "chinese_only_train=get_chinese_only_from_train(train_file)\n",
    "_1_gram_word_counts, _2_gram_word_counts=process_train(chinese_only_train)\n",
    "print ('probability of sentence: ', c1_sentences[0], 'is ' , two_gram_model(c1_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(sentence_list): \n",
    "    scores={}\n",
    "    for i in sentence_list:\n",
    "        scores[i]=two_gram_model(i)\n",
    "        sorted_scores=sorted(scores.items(), key=lambda x: x[1],reverse=True)\n",
    "    print ('sorted scores for those sentences are: ', sorted_scores)\n",
    "    print ('The sentence has the highest probability is :')\n",
    "    return sorted_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use movie_review as train， find teh best sentece in c1_sentences："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted scores for those sentences are:  [('我很讨厌这部电影显瘦', 8.336518735817308e-08), ('他特别喜欢这个餐馆显瘦', 9.673390997533045e-09), ('他太认为这部电影', 4.1429206936430063e-10), ('我特别认为这件衣服好用', 9.289250643850492e-11), ('我朋友很喜欢这个餐馆好用', 6.333332742196904e-11), ('他很想这个火锅显瘦', 5.0352895162167845e-11), ('你一点也不喜欢这个手机', 9.809868146471854e-12), ('她太认为这部电影美味', 4.912554182185384e-12), ('我朋友一点也不喜欢这个餐馆显瘦', 2.2310139412968292e-13), ('我朋友太想这个手机好看', 2.2771437718104992e-14), ('我妈妈特别喜欢这个火锅精彩', 5.163673408504117e-15), ('我爸爸一点也不想这个餐馆显瘦', 4.914003784687489e-15), ('我爸爸特别喜欢这件衣服好吃', 4.604821104012249e-15), ('我一点也不认为这个手机美味', 4.118373402282527e-15), ('我太觉得这个手机好看', 3.4474920377098815e-15), ('我爸爸很喜欢这件衣服好看', 2.69210488047519e-15), ('我爸爸特别想这件衣服精彩', 9.606472574414685e-17), ('我爸爸很觉得这个餐馆好看', 2.4764414496253318e-17), ('我朋友很觉得这个火锅精彩', 4.804912675011442e-18), ('我爸爸一点也不认为这个手机精彩', 1.1375624871217606e-20)]\n",
      "The sentence has the highest probability is :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('我很讨厌这部电影显瘦', 8.336518735817308e-08)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best(c1_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use insurance text as train，find the best sentecne in c2_sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 Chinese characters of the train file is : 法律要求残疾保险吗债权人可以在死后人寿保\n",
      "Length of cleaned train file:  145084\n",
      "Train file contains  74259  of words(tokens).\n",
      "sorted scores for those sentences are:  [('先生本款保单卖地不错', 0.16666666666666666), ('你这个项目投资收益高', 0.025), ('王老板本款保单投资收益高', 0.008333333333333333), ('王老板本款保单投资不错', 0.001984126984126984), ('你本款保单投资不错', 0.001984126984126984), ('你这个产品真的风险低', 0.001633986928104575), ('先生这个项目购买不错', 0.0014005602240896359), ('女士这个产品购买人多', 0.0014005602240896359), ('王老板本款保单真的风险低', 0.0005446623093681917), ('你这个项目真的收益高', 0.0004901960784313725), ('王老板本款保单购买不错', 0.0004668534080298786), ('你这个项目购买回报稳定', 0.0002334267040149393), ('王老板这个产品投资很好', 2.3761999809904e-05), ('先生这个项目购买收益高', 2.334267040149393e-05), ('你这款保险卖地风险低', 1.11000111000111e-05), ('先生这款保险购买风险低', 2.0728312044838654e-08), ('女士这款保险真的很好', 7.819662627693625e-09), ('先生这款保险真的保险', 3.9137372392352707e-10), ('王老板这款保险购买保险', 3.727368799271686e-10)]\n",
      "The sentence has the highest probability is :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('先生本款保单卖地不错', 0.16666666666666666)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file='./train_insurance.txt'\n",
    "chinese_only_train=get_chinese_only_from_train(train_file)\n",
    "_1_gram_word_counts, _2_gram_word_counts=process_train(chinese_only_train)\n",
    "#print ('probability of sentence: ', c2_sentence10[0], 'is ' , two_gram_model(c2_sentence10[0]))\n",
    "generate_best(c2_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "1） 长的句子可能普遍比短的句子的概率要小<br>\n",
    "2） 两个词库相对来说都是比较专门的词库，一个关于电影，一个关于保险，所以句子中出现高频词（电影，保险）的句子概率会比较大。<br>\n",
    "\n",
    "N-Gram: Advantages:<br>\n",
    "Encode not just keywords, but also word ordering, automatically<br>\n",
    "Models are not biased by hand coded lists of words, but are completely dependent on real data<br>\n",
    "Learning features of each affect type is relatively fast and easy<br>\n",
    "\n",
    "N-Gram: Disadvantages<br>\n",
    "Long range dependencies are not captured<br>\n",
    "Dependent on having a corpus of data to train from<br>\n",
    "Sparse data for low frequency affect tags adversely affects the quality of the n-gram model<br>\n",
    "\n",
    "改进方法：<br>\n",
    "Weighted Approach<br>\n",
    "Weight the longer n-grams higher in the stochastic model<br>\n",
    "Lengths Approach<br>\n",
    "Include a length-of-utterances factor, capturing the differences in utterance length between affect tags<br>\n",
    "Weights with Lengths Approach<br>\n",
    "Combine Weighted with Lengths<br>\n",
    "Repetition Approach<br>\n",
    "Combine all the above information, with overlap of words between thought units\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
