{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?\n",
    "\n",
    "An autoencoder is trying to learn and compress data in an unsupervised manner by training the data through models or network and on the other side, reconstruct the data from the reduced encoding that represents as close as possible to its original input. <br>\n",
    "\n",
    "Encoding部分学习并对原始数据降维，用更简单的coding来代表原始数据。Decoder 部分可以用降维过的encoding数据还原原始信息，而且保证信息基本不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?\n",
    "A greedy search algorithm always choose the locally optimal choices at each stage with the hope of finding a global optimum. <br>\n",
    "Beam search algorithm explores a graph by expanding a set of most promising nodes.<br>\n",
    "greedy search 每次在每一个step只保留最好的结果，然后根据这个结果找下一个最优结果。 <br>\n",
    "Beam search会保留几个top的最优解，然后对这些最优解都进行下一步的search，从而找到最终的最优解。<br>\n",
    "在NLP中有时候在sequence to sequence 的model中，并不是所有时候第一个词最好就可以保证后面的句子最好，所以在这种情况下Beam Search 会比greedy search 产生更好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?\n",
    "RNN或者LSTM都很难解决长程联系. 在一般的decoder的过程中，我们会用整个encoder的句子来预测decoder的第一词，这样子会造成信息冗余。Attention可以用局部聚焦解决这个问题。\n",
    "Attention会有一个context matrix， 对encoder部分不同时间点的隐状态加上不同的权重，从而来加强在decoder时预测对应位置的词的效果。 Context matrix 的一种算法是用softmax function， 对每一个时间点的隐状态得到一个weight， 然后乘以encoder 阶段每个时间点的隐状态vector。因为attention 还是用到了所有时间点的隐状态，所以同时还可以解决长程联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?\n",
    "用CBOW， TFIDF，skip-gram， Word2vec 所得到的的词向量是从**全部的语料库**得来的，对每个词都是**单一且固定**的，所以无法解决**同一个词在不同句子中一词多义**的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)\n",
    "\n",
    "ELMo的结构是一个双层双向的LSTM模型。它通过预测句子中下一个词来训练自己这个模型，所以不需要标记的数据。 每一层都会有一个前向的和反向的预测出的词向量的输出，然后把每一层输出的词向量vector表示按照权重相加。权重是通过模型学习得到。所以通过ELMo预测得到的词向量，会根据**每一句话考虑上下语境(LSTM)**。\n",
    "\n",
    "ELMo的两种用法： <br>\n",
    "1）已经训练好一个ELMo模型，然后输入所需句子用传统方式得到的Word Embedding从而输出的ElMo表示的word Embedding形式，该形式的Word Embedding就会考虑到该句子的具体语境。还可以根据结果对现有ELMo模型的参数进行fine tune。<br>\n",
    "2）还是利用以后的ELMo 模型+传统的word embedding，得到一个 ELMo形式的词向量，然后对ELMo词向量和传统的word embedding 进行拼接，输入预测任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?\n",
    "Transformer 有self-attention机制，同时又能解决长程联系。Transformer 可以并行计算，RNN要用到之前的input，所以无法并行计算。但是transformer的缺点是输入的长度必须是固定的。\n",
    "\n",
    "Transformer主要结构由Attention 和 全连接网络构成。原始paper里用transformer来做机器翻译，encoder部分用了self-attention 加上layer normalization。\n",
    "\n",
    "Decoder部分的结构很深，原始paper里的大decoder 部分有6个小encoder， 每个小encoder 都要输入到6个小decoder 部分。 然后用全连接网络链接。FN部分采用的是ResNet，从而解决梯度消失的问题。 \n",
    "\n",
    "\n",
    "encoder part: word vector -> (+ position vector) -> self-attention layer -> Resnet(add and normalization)->  FC layer-> Resnet (add and normalization)\n",
    "\n",
    "Decoder part: self-attention layer -> Resnet(add and normalization)-> encoder decoder attention\n",
    "-> Resnet(add and normalization)->  FC layer -> Resnet(add and normalization)\n",
    "\n",
    "Attention= ((Q*KT)/ sqrt(k)) * V  \n",
    "\n",
    "sqrt(k) will make the training more stable\n",
    "\n",
    "K and V come from encoder and  Q comes from decoder\n",
    "\n",
    "Transformer在翻译对话的时候，是input第一个词跑一遍翻译出第一个词，然后拿预测出的第一个结果和input的第二个词再跑一遍预测出第二个...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?\n",
    "Batch normalization 是对一个样本set中相同位置的词的词向量的**某个维度**进行标准化。但是不同句子的第一个词是任意的，每个句子中第二个词之间也没有任何关系，所以用batch normalization对sequence to sequence 的NLP 任务没有意义。\n",
    "\n",
    "Layer normalization是对一个batch里面一个样本(句子）中某个位置所含的词**（某一个词）**向量的标准化。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?\n",
    "\n",
    "因为self attention 产生的词向量是本来的词向量的一个加权，如果不加position embedding，那么语料中（比如是”我爱“  Vs “爱我”）的效果是一样的， 词语相对位置的信息就丢失了。 加上position embedding可以解决这个问题。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?\n",
    "\n",
    "Self-attention是transfomer encoder部分将原有的和上下文无关的X1，X2 词向量转换成Z1，Z2的过程。具体做法是每一个词会有一个embedding通过query之后每个词会变成Q (query)， K (keys)， V (values) 三个vector。 和attention类似，用softmax function 得到第一个词对V1， V2的权重，然后用相对应的权重乘以V1， V2 得到新的Z1. 在这个转换过程中，self-attention 会考虑到两个词之间的关系，从而解决一些时序性的问题。 \n",
    "\n",
    "Multi-head attention是指用若干个queries来对X得到多个Z， 然后将得到的Z相连接，通过网络得到新的一个Z.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?\n",
    "就是transformer里面的简化过的decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?\n",
    "\n",
    "GPT可以用于text生成也可以用于text分类。<br>\n",
    "text生成的例子： 对话系统<br>\n",
    "text分类的例子： <br>\n",
    "1)用一句话或若干句话生成一个结果：sentiment analysis <br>\n",
    "2)通过一个词给出一个结果： 比如句法标注，实体识别\n",
    "\n",
    "GPT是单向的 （mask所有后面的词），Bert是双向的（mask 单个词）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?\n",
    "输入中的百分之15的词会进行特殊处理，其中80%用mask替换，10%用其他词进行随机替换，10%不变，然后用模型来预测这些特殊处理过的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?\n",
    "\n",
    "BERT是通过transformer的encoder 堆叠起来的。\n",
    "Bert的 input 包过Token embeddings，Segment embeddings，Position Embeddings, 然后将这三个进行加和，得到一个输入。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks.\n",
    "\n",
    "BERT的原始output 是一个sentence-level 或者 token-level的向量。它可以被用来做text生成， 或者再加一层classification之后就可以用bert model来做text的分类。\n",
    "\n",
    "BERT is a model that knows to represent text. You give it some sequence as an input, it then looks left and right several times and produces a vector representation for each word as the output.\n",
    "\n",
    "BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model: Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the token。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT是单项的transfomer，BERT是双向的transformer。\n",
    "\n",
    "GPT2 用更多的数据，结构更深。\n",
    "\n",
    "ELMo是双向的LSTM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
