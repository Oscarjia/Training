{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is independent assumption in Naive bayes ?\n",
    "Naive Bayes assume that the effect of the value of a predictor (x) on a given class (c) is independent of the values of other predictors. This assumption is called class conditional independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is MAP(maximum a posterior) and ML(maximum likelihood) ?\n",
    "One way to obtain a point estimate is to choose the value of x that maximizes the posterior PDF (or PMF). This is called the maximum a posteriori (MAP) estimation. Maximum a posteriori (MAP) estimate of X given Y=y is the value of x that maximizes the posterior PDF。\n",
    "If the prior probability is the same for all the hypothesis then MAP = ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is support vector in SVM?\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the intuition behind SVM ?\n",
    "Find the boundary for classification that has the maximum margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Shortly describ what 'random' means in random forest ?\n",
    "Random in random forest means use a random subset of samples and a random sample of features in each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What cariterion does XGBoost use to find the best split point in a tree ?\n",
    "maximuze the gain 最大化信息增益 <br>\n",
    "https://arxiv.org/pdf/1603.02754.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Practial part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem description: In this part you are going to build a classifier to detect if a piece of news is published by the Xinhua news agency (新华社）."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints:\n",
    "###### 1. Firstly, you have to come up with a way to represent the news. (Vectorize the sentence, you can find different ways to do so online)  \n",
    "###### 2. Secondly,  pick a machine learning algorithm that you think is suitable for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89611\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "\n",
    "path='/Users/zixiawang/school/kaikeba/作业笔记/开课吧NLP/Project/Project1/Unsupervise_news_abstract/raw_data'\n",
    "df=pd.read_csv(path+'/sqlResult_1558435.csv',encoding='GB18030')\n",
    "print (len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>新华社</td>\n",
       "      <td>0.877825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>微博</td>\n",
       "      <td>0.027642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>中国证券报?中证网</td>\n",
       "      <td>0.006004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>中国新闻网</td>\n",
       "      <td>0.005859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>参考消息网</td>\n",
       "      <td>0.004296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index    source\n",
       "0        新华社  0.877825\n",
       "1         微博  0.027642\n",
       "2  中国证券报?中证网  0.006004\n",
       "3      中国新闻网  0.005859\n",
       "4      参考消息网  0.004296"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source=df.source.value_counts(2).to_frame().reset_index()\n",
    "source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.87795\n",
       "0    0.12205\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y']=df.apply(lambda x:  1 if len(re.findall('新华社',str(x.source)))>0  else 0, axis=1)\n",
    "#lambda x : True if (x > 10 and x < 20) else False\n",
    "df['y'].value_counts(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_content=df[df['content'].isnull()]\n",
    "null_content.y.value_counts(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all news with content is NAN are not published by 新华社, drop them from the research dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89611\n",
      "87054\n",
      "87054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.903738\n",
       "0    0.096262\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (len(df))\n",
    "df=df[~df['content'].isnull()]\n",
    "print (len(df))\n",
    "df=df[~df['title'].isnull()]\n",
    "print (len(df))\n",
    "df['y'].value_counts(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85% for training 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.853562\n",
       "0    0.146438\n",
       "Name: train, dtype: float64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train']=df.apply(lambda x: 0 if random.uniform(0, 1)<=0.15 else 1, axis=1)\n",
    "df['train'].value_counts(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.90387\n",
      "0    0.09613\n",
      "Name: y, dtype: float64\n",
      "1    0.902965\n",
      "0    0.097035\n",
      "Name: y, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print (df[df['train']==1].y.value_counts(2))\n",
    "print (df[df['train']==0].y.value_counts(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_content']=df.apply(lambda x: str(x.title) +' '+ str(x.content).replace('\\r\\n', ''), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    87054.000000\n",
       "mean       466.596411\n",
       "std        664.113557\n",
       "min          5.000000\n",
       "25%        156.000000\n",
       "50%        202.000000\n",
       "75%        521.000000\n",
       "max      17529.000000\n",
       "Name: news_length, dtype: float64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['news_length']=df.title_content.apply(lambda x: len(str(x)))\n",
    "df['news_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'小米MIUI 9首批机型曝光：共计15款 此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/体验版内测，稳定版暂不受影响），以确保工程师可以集中全部精力进行系统优化工作。有人猜测这也是将精力主要用到MIUI 9的研发之中。MIUI 8去年5月发布，距今已有一年有余，也是时候更新换代了。当然，关于MIUI 9的确切信息，我们还是等待官方消息。'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>feature</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>y</th>\n",
       "      <th>train</th>\n",
       "      <th>title_content</th>\n",
       "      <th>news_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>快科技@http://www.kkj.cn/</td>\n",
       "      <td>此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/...</td>\n",
       "      <td>{\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"37\"...</td>\n",
       "      <td>小米MIUI 9首批机型曝光：共计15款</td>\n",
       "      <td>http://www.cnbeta.com/articles/tech/623597.htm</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>小米MIUI 9首批机型曝光：共计15款 此外，自本周（6月12日）起，除小米手机6等15款...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>快科技@http://www.kkj.cn/</td>\n",
       "      <td>骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考...</td>\n",
       "      <td>{\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"15\"...</td>\n",
       "      <td>骁龙835在Windows 10上的性能表现有望改善</td>\n",
       "      <td>http://www.cnbeta.com/articles/tech/623599.htm</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>骁龙835在Windows 10上的性能表现有望改善 骁龙835作为唯一通过Windows ...</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>快科技@http://www.kkj.cn/</td>\n",
       "      <td>此前的一加3T搭载的是3400mAh电池，DashCharge快充规格为5V/4A。\\r\\n...</td>\n",
       "      <td>{\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"18\"...</td>\n",
       "      <td>一加手机5细节曝光：3300mAh、充半小时用1天</td>\n",
       "      <td>http://www.cnbeta.com/articles/tech/623601.htm</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>一加手机5细节曝光：3300mAh、充半小时用1天 此前的一加3T搭载的是3400mAh电池...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>新华社</td>\n",
       "      <td>这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\\r\\n</td>\n",
       "      <td>{\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...</td>\n",
       "      <td>葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）</td>\n",
       "      <td>http://world.huanqiu.com/hot/2017-06/10866126....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图） 这是6月18日在葡萄牙中部大佩德...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89613</td>\n",
       "      <td>胡淑丽_MN7479</td>\n",
       "      <td>深圳大件事</td>\n",
       "      <td>（原标题：44岁女子跑深圳约会网友被拒，暴雨中裸身奔走……）\\r\\n@深圳交警微博称：昨日清...</td>\n",
       "      <td>{\"type\":\"新闻\",\"site\":\"网易热门\",\"commentNum\":\"978\",...</td>\n",
       "      <td>44岁女子约网友被拒暴雨中裸奔 交警为其披衣相随</td>\n",
       "      <td>http://news.163.com/17/0618/00/CN617P3Q0001875...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44岁女子约网友被拒暴雨中裸奔 交警为其披衣相随 （原标题：44岁女子跑深圳约会网友被拒，暴...</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      author                  source  \\\n",
       "0  89617         NaN  快科技@http://www.kkj.cn/   \n",
       "1  89616         NaN  快科技@http://www.kkj.cn/   \n",
       "2  89615         NaN  快科技@http://www.kkj.cn/   \n",
       "3  89614         NaN                     新华社   \n",
       "4  89613  胡淑丽_MN7479                   深圳大件事   \n",
       "\n",
       "                                             content  \\\n",
       "0  此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/...   \n",
       "1  骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考...   \n",
       "2  此前的一加3T搭载的是3400mAh电池，DashCharge快充规格为5V/4A。\\r\\n...   \n",
       "3    这是6月18日在葡萄牙中部大佩德罗冈地区拍摄的被森林大火烧毁的汽车。新华社记者张立云摄\\r\\n   \n",
       "4  （原标题：44岁女子跑深圳约会网友被拒，暴雨中裸身奔走……）\\r\\n@深圳交警微博称：昨日清...   \n",
       "\n",
       "                                             feature  \\\n",
       "0  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"37\"...   \n",
       "1  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"15\"...   \n",
       "2  {\"type\":\"科技\",\"site\":\"cnbeta\",\"commentNum\":\"18\"...   \n",
       "3  {\"type\":\"国际新闻\",\"site\":\"环球\",\"commentNum\":\"0\",\"j...   \n",
       "4  {\"type\":\"新闻\",\"site\":\"网易热门\",\"commentNum\":\"978\",...   \n",
       "\n",
       "                           title  \\\n",
       "0           小米MIUI 9首批机型曝光：共计15款   \n",
       "1     骁龙835在Windows 10上的性能表现有望改善   \n",
       "2      一加手机5细节曝光：3300mAh、充半小时用1天   \n",
       "3  葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图）   \n",
       "4       44岁女子约网友被拒暴雨中裸奔 交警为其披衣相随   \n",
       "\n",
       "                                                 url  y  train  \\\n",
       "0     http://www.cnbeta.com/articles/tech/623597.htm  0      1   \n",
       "1     http://www.cnbeta.com/articles/tech/623599.htm  0      1   \n",
       "2     http://www.cnbeta.com/articles/tech/623601.htm  0      1   \n",
       "3  http://world.huanqiu.com/hot/2017-06/10866126....  1      1   \n",
       "4  http://news.163.com/17/0618/00/CN617P3Q0001875...  0      1   \n",
       "\n",
       "                                       title_content  news_length  \n",
       "0  小米MIUI 9首批机型曝光：共计15款 此外，自本周（6月12日）起，除小米手机6等15款...          192  \n",
       "1  骁龙835在Windows 10上的性能表现有望改善 骁龙835作为唯一通过Windows ...          346  \n",
       "2  一加手机5细节曝光：3300mAh、充半小时用1天 此前的一加3T搭载的是3400mAh电池...          234  \n",
       "3  葡森林火灾造成至少62人死亡 政府宣布进入紧急状态（组图） 这是6月18日在葡萄牙中部大佩德...           73  \n",
       "4  44岁女子约网友被拒暴雨中裸奔 交警为其披衣相随 （原标题：44岁女子跑深圳约会网友被拒，暴...         1621  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a gensim model for word embedding using text in  title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zixiawang/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#df['title_content'].to_csv('../segment/news_text.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "## clean up the segement text before count the frequency\n",
    "\n",
    "def get_chinese_number_letters(train_file):\n",
    "    train = train_file\n",
    "    train = open(train,encoding=\"utf8\", errors='ignore').read().lower()\n",
    "    #[\\u00C0-\\u017F] Unicode range for all Latin characters\n",
    "    #[\\u4e00-\\u9fff] Unicode range for all chinese characters\n",
    "    chinese_only=re.findall(r'([\\u4e00-\\u9fff]+|[A-Z]+|[0-9]+|[a-z]+| [ ])',train)\n",
    "    \n",
    "    #chinese_only=re.findall('[\\w\\s]',train)\n",
    "    print ('First 20 characters of the train file is :', ' '.join(chinese_only)[:20])\n",
    "    return ' '.join(chinese_only)\n",
    "\n",
    "def get_1_gram_count(text):\n",
    "    print ('Length of cleaned train file: ', len(text))\n",
    "    TOKENS = text.split(' ')\n",
    "    print ('Train file contains ', len(TOKENS), ' of words(tokens).')\n",
    "    _1_gram_word_counts = Counter(TOKENS)\n",
    "    return _1_gram_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 characters of the train file is : 小米 miui 9 首批机型曝光 共计 \n",
      "Length of cleaned train file:  40461622\n",
      "Train file contains  6032982  of words(tokens).\n",
      "My program took 4.5439441204071045 to run\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "train_file='../raw/news_text.txt'\n",
    "clean_file=get_chinese_number_letters(train_file)\n",
    "_1_gram_word_counts=get_1_gram_count(clean_file)\n",
    "print (\"My program took\", time.time() - start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"../segment/news_segment.txt\", \"w\")\n",
    "text_file.write(clean_file)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My program took 34.26250696182251 to run\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "\n",
    "# 如果目录中有多个文件，可以使用PathLineSentences\n",
    "sentences = word2vec.PathLineSentences('../segment/')\n",
    "# 设置模型参数，进行训练\n",
    "import time\n",
    "start_time = time.time()\n",
    "model = word2vec.Word2Vec(sentences, size=300, window=5, min_count=2,workers=multiprocessing.cpu_count())\n",
    "print (\"My program took\", time.time() - start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My program took 17.5863139629364 to run\n"
     ]
    }
   ],
   "source": [
    "## use model = word2vec.Word2Vec.load(\"../models/wiki_news.model\") to load the model if you need to re-train the model\n",
    "# 将Word转换成Vec，然后计算相似度 \n",
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "\n",
    "# 如果目录中有多个文件，可以使用PathLineSentences\n",
    "sentences = word2vec.PathLineSentences('../segment/')\n",
    "# 设置模型参数，进行训练\n",
    "import time\n",
    "start_time = time.time()\n",
    "model = word2vec.Word2Vec(sentences, size=100, window=5, min_count=5,workers=multiprocessing.cpu_count())\n",
    "print (\"My program took\", time.time() - start_time, \"to run\")\n",
    "model.save('../models/news.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83535"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('茶园采茶忙', 0.8049883842468262),\n",
       " ('中科院沈阳自动化研究所科研团队创新实录', 0.7998490929603577),\n",
       " ('花海漫步', 0.798293948173523),\n",
       " ('透视百色说脱贫', 0.7979933023452759),\n",
       " ('上图为', 0.7973495721817017),\n",
       " ('水云龙', 0.7942481637001038),\n",
       " ('情书', 0.7933979034423828),\n",
       " ('黄大年', 0.7933173179626465),\n",
       " ('百条街道花成海', 0.79017174243927),\n",
       " ('丝绸之路上的西安港是无', 0.786495566368103)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('妈妈')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('现实中', 0.9711506962776184),\n",
       " ('特种部队', 0.9706164598464966),\n",
       " ('误导消费者', 0.97043377161026),\n",
       " ('申报', 0.9696590304374695),\n",
       " ('你怎么看', 0.9691023826599121),\n",
       " ('合理控制同业存单等同业融资规模', 0.9685747027397156),\n",
       " ('推进学习教育常态化制度化', 0.9679213762283325),\n",
       " ('令', 0.9673484563827515),\n",
       " ('被告人', 0.967084527015686),\n",
       " ('研究中心', 0.9664682149887085)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('美女')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['小米', 'miui', '9', '共计', '15', '6', '月', '12', '日', '起']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.wv.vocab.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45370743\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('中国', '美国'))\n",
    "#save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以看出只用news 数据做的model 的词向量效果是不行的，于是采用project1 里面已经train 好的wiki+news 的model来做sentences embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取用在project 1 中wiki和news语料build的中文模型\n",
      "中文模型读取完毕\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import data_io, params, SIF_embedding\n",
    "weightpara = 1e-5  # the parameter in the SIF weighting scheme, usually in the range [3e-5, 3e-3]\n",
    "print('读取用在project 1 中wiki和news语料build的中文模型')\n",
    "words_chi, We_chi = data_io.getWordmap('../models/wiki_news_model2_vector.txt')\n",
    "word2weight_chi = data_io.getWordWeight('../models/word_count.txt',  # each line is a word and its frequency,\n",
    "                                                  weightpara)  # word2weight['str'] is the weight for the word 'str'\n",
    "print('中文模型读取完毕')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight4ind finished \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from scipy.stats import pearsonr\n",
    "import re\n",
    "from scipy.stats.stats import pearsonr\n",
    "sys.path.append('../src')\n",
    "import data_io, params, SIF_embedding\n",
    "words = words_chi\n",
    "word2weight =word2weight_chi\n",
    "We = We_chi\n",
    "rmpc = 1  # number of principal components to remove in SIF weighting scheme\n",
    "weight4ind = data_io.getWeight(words, word2weight)\n",
    "print('weight4ind finished ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentences):\n",
    "    #sentences=[df.title_content]\n",
    "    # load sentences\n",
    "    x, m = data_io.sentences2idx_c(sentences, words)\n",
    "    print('sentences2idx finished ')\n",
    "    w = data_io.seq2weight(x, m, weight4ind)  # get word weights\n",
    "    print('seq2weight finished ')\n",
    "    # set parameters\n",
    "    param = params.params()\n",
    "    param.rmpc = rmpc\n",
    "    # get SIF embedding\n",
    "    \"\"\"\n",
    "    return 所有需要计算similarity的title，全文，句子的embedding。\n",
    "    paper 里面用的是TruncatedSVD，project 要求我们用PCA方法decomposite\n",
    "    \"\"\"\n",
    "    print('embedding start ')\n",
    "    embedding = SIF_embedding.SIF_embedding(We, x, w, param,\n",
    "                                                method='PCA')  # embedding[i,:] is the embedding for sentence i\n",
    "    print('embedding finished ')\n",
    "    print( embedding.shape)\n",
    "    #df['news_embedding']=embedding\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74306\n",
      "12748\n"
     ]
    }
   ],
   "source": [
    "train_sentences=[]\n",
    "test_sentences=[]\n",
    "for i in df[df.train==1]['title_content']:\n",
    "    train_sentences.append(i)\n",
    "print (len(train_sentences))\n",
    "for i in df[df.train==0]['title_content']:\n",
    "    test_sentences.append(i)\n",
    "print (len(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embedding for train sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences2idx finished \n",
      "seq2weight finished \n",
      "embedding start \n",
      "emb shape: (74306, 100)\n",
      "PCA used for decompostion PCA(copy=True, iterated_power='auto', n_components=1, random_state=0,\n",
      "    svd_solver='auto', tol=0.0, whiten=False)\n",
      "embedding finished \n",
      "(74306, 100)\n",
      "My program took 1184.2459480762482 to run\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "X_train=get_embedding(train_sentences)\n",
    "print (\"My program took\", time.time() - start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embedding for test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences2idx finished \n",
      "seq2weight finished \n",
      "embedding start \n",
      "emb shape: (12748, 100)\n",
      "PCA used for decompostion PCA(copy=True, iterated_power='auto', n_components=1, random_state=0,\n",
      "    svd_solver='auto', tol=0.0, whiten=False)\n",
      "embedding finished \n",
      "(12748, 100)\n",
      "My program took 200.90599513053894 to run\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "X_test=get_embedding(test_sentences)\n",
    "print (\"My program took\", time.time() - start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74306, 100) (12748, 100)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74306,) (12748,)\n"
     ]
    }
   ],
   "source": [
    "y_train=df[df.train==1]['y']\n",
    "y_test=df[df.train==0]['y']\n",
    "print (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use classic Logistic Regression Model with L2正则\n",
    "先尝试最经典，比较robust的逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 3.1622776601683795}\n",
      "{'C': 3.1622776601683795}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9659    0.9846    0.9752      1237\n",
      "           1     0.9983    0.9963    0.9973     11511\n",
      "\n",
      "    accuracy                         0.9951     12748\n",
      "   macro avg     0.9821    0.9905    0.9862     12748\n",
      "weighted avg     0.9952    0.9951    0.9952     12748\n",
      "\n",
      "My program took 26.743412971496582 to run\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report  #  这个用来打印最终的结果，包括F1-SCORE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params_c = np.logspace(-5,2,15) # 也可以自行定义一个范围\n",
    "\n",
    "# 逻辑回归 + L2正则， 利用GrisSearchCV\n",
    "parameter_grid = {'C': params_c}\n",
    "model= LogisticRegression(penalty='l2').fit(X_train, y_train)\n",
    "grid_search = GridSearchCV(model,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=4, n_jobs=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "# 输出最好的参数 \n",
    "print(grid_search.best_params_)\n",
    "\n",
    "y_test_predict = grid_search.predict(X_test)\n",
    "print (classification_report(y_test, y_test_predict,digits=4 ))\n",
    "\n",
    "print (\"My program took\", time.time() - start_time, \"to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1218    43]\n",
      " [   19 11468]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print (confusion_matrix(y_test_predict,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result based on logistic Regression: \n",
    "#### Accuracy is 99.52%,   F1 score is already pretty high at 99.73 % "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try tpot method\n",
    "Tpot对大的数据运行非常慢，而且Logistic Regression已经取得了很好的F1 score，提升的空间非常有限了，所以我没有等到全部generation run 完就停止了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8dea4b046f4ad08306f366538224d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=110, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.9892875867033976\n",
      "Generation 2 - Current best internal CV score: 0.9913466276754643\n",
      "Generation 3 - Current best internal CV score: 0.9913466276754643\n",
      "Generation 4 - Current best internal CV score: 0.9913466276754643\n",
      "Generation 5 - Current best internal CV score: 0.9920094830043561\n",
      "Generation 6 - Current best internal CV score: 0.9920094830043561\n",
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: XGBClassifier(LogisticRegression(input_matrix, C=20.0, dual=False, penalty=l2), learning_rate=0.5, max_depth=4, min_child_weight=13, n_estimators=100, nthread=1, subsample=0.8)\n",
      "\n",
      "Best pipeline: XGBClassifier(LogisticRegression(input_matrix, C=20.0, dual=False, penalty=l2), learning_rate=0.5, max_depth=4, min_child_weight=13, n_estimators=100, nthread=1, subsample=0.8)\n",
      "0.9963915908377785\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=10, population_size=10, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "#tpot.export('tpot_news_classification.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tpot 取得的accuracy 是99.20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed all assignments in this week. The question below is optional. If you still have time, why don't try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try differnt machine learning algorithms with different combinations of parameters in the practical part, and compare their performances (Better use some visualization techiniques)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
