{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you code here\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import random\n",
    "dataset = load_boston()\n",
    "x,y=dataset['data'],dataset['target']\n",
    "print (x.shape)\n",
    "print (y.shape)\n",
    "dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11e6f5588>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#X的第五列\n",
    "X_rm = x[:,5]\n",
    "# plot the RM with respect to y\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(X_rm,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7406426641094094\n",
      "Linear regression coefficients from sklearn are: [-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n",
      "Linear regression intercept from sklearn is: 36.45948838508994\n"
     ]
    }
   ],
   "source": [
    "import icecream as ic\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(x, y)\n",
    "print (reg.score(x, y))\n",
    "print ('Linear regression coefficients from sklearn are:' , reg.coef_)\n",
    "print ('Linear regression intercept from sklearn is:', reg.intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN classifier and KNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0057030013836377735\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# you code here\n",
    "def model(x, y):\n",
    "    return [(xi, yi) for xi, yi in zip(x, y)]\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "def distance(x1, x2):\n",
    "    return cosine(x1, x2)\n",
    "\n",
    "print (distance(x[0], x[1]))\n",
    "print (distance(x[0], x[0]))\n",
    "\n",
    "from collections import Counter\n",
    "def predict(testInstance, type='regression', k=5):\n",
    "    distances = [distance(testInstance, i) for i in x]\n",
    "    #print (distances)\n",
    "    kneighbors = np.argsort(distances)[:k]\n",
    "    ###Returns the indices that would sort an array\n",
    "    #print (kneighbors)\n",
    "    count = Counter(y[kneighbors])\n",
    "    if type=='regression':\n",
    "        print (y[kneighbors])\n",
    "        return  print ('predicted value is: ' ,np.mean(y[kneighbors]))\n",
    "    else:\n",
    "        print (count)\n",
    "        return print ('predicted value is: ' , count.most_common()[0][0])\n",
    "#most_similars = sorted(model(X, y), key=lambda xi: distance(xi[0], x))[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  22.2 20.1 23.9 22.9]\n",
      "predicted value is:  22.620000000000005\n"
     ]
    }
   ],
   "source": [
    "predict(x[0], type='regression', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({24.0: 1, 22.2: 1, 20.1: 1, 23.9: 1, 22.9: 1})\n",
      "predicted value is:  24.0\n"
     ]
    }
   ],
   "source": [
    "predict(x[0], type='classification', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bought</th>\n",
       "      <th>family_number</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bought  family_number gender income\n",
       "0       1              1      F    +10\n",
       "1       1              1      F    -10\n",
       "2       1              2      F    +10\n",
       "3       0              1      F    +10\n",
       "4       0              1      M    +10\n",
       "5       0              1      M    +10\n",
       "6       1              2      M    -10"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you code here\n",
    "from collections import Counter\n",
    "def entropy(elements):\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in elements]\n",
    "    return - sum(p * np.log(p) for p in probs)\n",
    "\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "dataset = pd.DataFrame.from_dict(mock_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "def sort_feature_by_importance(training_data: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    feature_entropy={}\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        values = set(training_data[f])\n",
    "        ic(values)\n",
    "        for v in values:\n",
    "            ic(v)\n",
    "            sub_1 = training_data[training_data[f] == v][target]\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            ic(entropy_1)\n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            ic(entropy_2)\n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            ic(entropy_v)\n",
    "            feature_entropy[f]=entropy_v\n",
    "    sorted_features = dict(sorted(feature_entropy.items(), key=lambda x: x[1]))\n",
    "    print('Sort the features by salience:', sorted_features)\n",
    "    return sorted_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| v: 'F'\n",
      "ic| entropy_1: 0.9938582532964797\n",
      "ic| entropy_2: 0.9068242403669224\n",
      "ic| entropy_v: 1.9006824936634021\n",
      "ic| v: 'M'\n",
      "ic| entropy_1: 0.9068242403669224\n",
      "ic| entropy_2: 0.9938582532964797\n",
      "ic| entropy_v: 1.9006824936634021\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| v: '-10'\n",
      "ic| entropy_1: -0.0\n",
      "ic| entropy_2: 1.6525187082781072\n",
      "ic| entropy_v: 1.6525187082781072\n",
      "ic| v: '+10'\n",
      "ic| entropy_1: 1.6525187082781072\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 1.6525187082781072\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| v: 1\n",
      "ic| entropy_1: 1.6525187082781072\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 1.6525187082781072\n",
      "ic| v: 2\n",
      "ic| entropy_1: -0.0\n",
      "ic| entropy_2: 1.6525187082781072\n",
      "ic| entropy_v: 1.6525187082781072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort the features by salience: {'income': 1.6525187082781072, 'family_number': 1.6525187082781072, 'gender': 1.9006824936634021}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['income', 'family_number', 'gender'])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_feature_by_importance(dataset, 'bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11d2aedd8>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X = [random.randint(0, 100) for _ in range(100)]\n",
    "Y = [random.randint(0, 100) for _ in range(100)]\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "    n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tranning_data = [[x, y] for x, y in zip(X, Y)]\n",
    "cluster = KMeans(n_clusters=6, max_iter=500)\n",
    "cluster.fit(tranning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57.2        79.48      ]\n",
      " [48.63636364 14.54545455]\n",
      " [16.73333333 32.53333333]\n",
      " [90.57142857 79.5       ]\n",
      " [84.         24.        ]\n",
      " [16.9375     76.625     ]]\n",
      "[5 0 4 3 2 4 0 0 1 2 5 2 1 4 4 3 2 0 3 2 3 1 1 5 4 4 0 2 3 5 0 3 0 3 0 0 0\n",
      " 4 5 2 1 0 4 0 0 0 0 5 4 0 0 4 1 2 4 5 0 5 0 5 5 3 4 5 0 4 1 0 2 1 4 3 0 1\n",
      " 1 2 5 3 2 2 4 4 0 4 3 3 2 5 2 5 5 4 2 5 1 3 0 4 0 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (cluster.cluster_centers_)\n",
    "print (cluster.labels_)\n",
    "from collections import defaultdict\n",
    "centers = defaultdict(list)\n",
    "for label, location in zip(cluster.labels_, tranning_data):\n",
    "    centers[label].append(location)\n",
    "color = ['red', 'green', 'grey', 'black', 'yellow', 'orange']\n",
    "for i, c in enumerate(centers):\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location, c=color[i])\n",
    "        \n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center, s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: a model is trying to simplify a problem and get answer by observe historical data.<br>\n",
    "All models are wrong, because we are trying to simpliy the problem so we might miss some potential factors. But we say the model is useful since some good models include top factors in the mode, so the prediction will be close to ground truth.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <br>\n",
    "Underfitting is accuracy of the model is low in both training and test dataset. \n",
    "The model is too simple that did not capture the full relationship between X and Y. Additional Xs or transformation of Xs need to be added to the model, or using a more complicated algorithm for fitting.\n",
    "\n",
    "\n",
    "Overfitting is the model's accuracy is high for the training dataset but low for the test dataset.\n",
    "The cause for overfitting could be:  <br>\n",
    "1) the model is too smiple, not complicated enought to capture the full relationship <br>\n",
    "2) The training dataset is too small <br>\n",
    "3) The training dataset is only in certain range, while the test or the dataset needs to be predicted are in a different rage <br>\n",
    "4) The parameters for the model is too big <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: <br>\n",
    "**Precision** = True Positive / (True Positive + False Positive) = True Positive /  All positive predicted <br>\n",
    "\n",
    "**Recall** = True Positive / (True Positive + False negative) = True positive / All positive in ground truth  <br>\n",
    "\n",
    "**F1 score** =  2* (Precision * recall) / (Precision +recall ) \n",
    "\n",
    "**AUC:** ROC curve is a probability curve that measure the performance of classification problem. AUC means area under the ROC curve, which measures the degree of separability. The x-axis of ROC curve is False positive rate (1 - Specificity ). The y-axis of ROC curve is True positive rate ( also called sensitivity) \n",
    "\n",
    "**Sensitivity **(also called true positive rate) = True postive / ( True positive + False negative) <br>\n",
    "\n",
    "**Specificity **(also called true negative rate) = True Negative / ( True Negative + False Positive) <br>\n",
    "\n",
    "**F2 score:**<br>\n",
    "The general formula for the F-score is the following:\n",
    "\n",
    "𝐹 score =(1+β2)⋅𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛⋅𝑟𝑒𝑐𝑎𝑙𝑙 /((β2⋅𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛)+𝑟𝑒𝑐𝑎𝑙𝑙)\n",
    "where β is a positive real. For the F2 score, you just set β equal to 2. The intuition behind the F2 score is that it weights recall higher than precision. This makes the F2 score more suitable in certain applications where it’s more important to classify correctly as many positive samples as possible, rather than maximizing the number of correct classifications.<br>\n",
    "\n",
    "Reference:\n",
    "https://en.wikipedia.org/wiki/Sensitivity_and_specificity <br>\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic <br>\n",
    "https://www.quora.com/What-is-the-F2-score-in-machine-learning <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Machine learning uses historical data try to find the relationship between independent variables and dependent variables or patterns in independent variablse themselves, it relies on statistical probability. <br>\n",
    "\n",
    "While classicial programming are more rule based. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性.\n",
    "\n",
    "I think the evaluatoin here means cost function. We need to define cost function as the first step of machine learning and either maxmize or minimize the function. When it applies to business, we also need to consider the cost of each wrong case (e.g. the cost of false positive and false negative might be quite different in different use case) while evaluating a model. \n",
    "\n",
    "So I agree that it solves half of the problem, the rest of problems remain on select a good model and hyperparamters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| v: 'F'\n",
      "ic| entropy_1: 0.9938582532964797\n",
      "ic| entropy_2: 0.9068242403669224\n",
      "ic| entropy_v: 1.9006824936634021\n",
      "ic| v: 'M'\n",
      "ic| entropy_1: 0.9068242403669224\n",
      "ic| entropy_2: 0.9938582532964797\n",
      "ic| entropy_v: 1.9006824936634021\n",
      "ic| f: 'income'\n",
      "ic| values: {'-10', '+10'}\n",
      "ic| v: '-10'\n",
      "ic| entropy_1: -0.0\n",
      "ic| entropy_2: 1.6525187082781072\n",
      "ic| entropy_v: 1.6525187082781072\n",
      "ic| v: '+10'\n",
      "ic| entropy_1: 1.6525187082781072\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 1.6525187082781072\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| v: 1\n",
      "ic| entropy_1: 1.6525187082781072\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 1.6525187082781072\n",
      "ic| v: 2\n",
      "ic| entropy_1: -0.0\n",
      "ic| entropy_2: 1.6525187082781072\n",
      "ic| entropy_v: 1.6525187082781072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort the features by salience: {'income': 1.6525187082781072, 'family_number': 1.6525187082781072, 'gender': 1.9006824936634021}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['income', 'family_number', 'gender'])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_features=sort_feature_by_importance(dataset, 'bought')\n",
    "sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case={'income': -10, 'family_number': 1, 'gender': 'M'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(dataset['bought']).most_common()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['income', 'family_number', 'gender'])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这一题不会做，没写出来，空着"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_predict(dataset):\n",
    "    sorted_features=sort_feature_by_importance(dataset, 'bought')\n",
    "    for i in sorted_features:\n",
    "        if len(dataset[i]==test_case[i])>0:\n",
    "            sub=dataset[dataset[i]==test_case[i]]\n",
    "            if ？？？\n",
    "        \n",
    "        else:\n",
    "            ##value of the test case for the 1st(most) important variable does not exist\n",
    "            sorted_features.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predict函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ loss = \\frac{1}{n} \\sum{|y_i - (kx_i + b_i)|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "#loss function based mean absolute difference\n",
    "def loss2(y,y_hat):\n",
    "    return sum(np.absolute(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{loss}}{\\partial{k}} =  -\\frac{1}{n}\\sum \\frac{y_i-\\hat{y_i}}{|y_i-\\hat{y_i}|}\\ x_i$$ \n",
    "\n",
    "$$ \\frac{\\partial{loss}}{\\partial{b}} = -\\frac{1}{n}\\sum \\frac{y_i-\\hat{y_i}}{|y_i-\\hat{y_i}|}\\ $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define partial derivative \n",
    "def partial_derivative_k(x, y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        gradient += (y_i-y_hat_i)/ np.absolute(y_i - y_hat_i) * x_i\n",
    "    return -1/n * gradient\n",
    "\n",
    "def partial_derivative_b(y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "        gradient += (y_i-y_hat_i)/np.absolute(y_i-y_hat_i)\n",
    "    return -1 / n * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update the paramters with -gradient * learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 489.47887247411074, parameters k is -83.84213831438174 and b is 59.97111940966735\n",
      "Iteration 1, the loss is 449.2587966326328, parameters k is -77.59816043469719 and b is 60.95007715461557\n",
      "Iteration 2, the loss is 412.3349436236702, parameters k is -71.86587225446502 and b is 61.84859474788084\n",
      "Iteration 3, the loss is 378.43717281691517, parameters k is -66.60333844882788 and b is 62.67326463512818\n",
      "Iteration 4, the loss is 347.3174828498916, parameters k is -61.77206048578569 and b is 63.43013898076201\n",
      "Iteration 5, the loss is 318.7481972130746, parameters k is -57.33669496520482 and b is 64.12477394646179\n",
      "Iteration 6, the loss is 292.52029853465643, parameters k is -53.26479504124194 and b is 64.76227034088794\n",
      "Iteration 7, the loss is 268.4418993783392, parameters k is -49.526573036389976 and b is 65.34731093795725\n",
      "Iteration 8, the loss is 246.33683836629163, parameters k is -46.0946825103956 and b is 65.88419473671392\n",
      "Iteration 9, the loss is 226.04339135628422, parameters k is -42.944018189631755 and b is 66.3768684134465\n",
      "Iteration 10, the loss is 207.41308824379362, parameters k is -40.05153229317884 and b is 66.82895519615907\n",
      "Iteration 11, the loss is 190.30962673261715, parameters k is -37.39606591182882 and b is 67.24378137264667\n",
      "Iteration 12, the loss is 174.60787512698545, parameters k is -34.95819420635595 and b is 67.6244006261119\n",
      "Iteration 13, the loss is 160.19295684944865, parameters k is -32.72008429250155 and b is 67.97361637636588\n",
      "Iteration 14, the loss is 146.95940998674007, parameters k is -30.665364772938343 and b is 68.29400229006478\n",
      "Iteration 15, the loss is 134.8104157147158, parameters k is -28.779005961690714 and b is 68.58792111003827\n",
      "Iteration 16, the loss is 123.65708995742841, parameters k is -27.047209924714906 and b is 68.8575419414677\n",
      "Iteration 17, the loss is 113.41783309799726, parameters k is -25.45730953215972 and b is 69.10485612138255\n",
      "Iteration 18, the loss is 104.01773298366318, parameters k is -23.99767578375902 and b is 69.33169178757855\n",
      "Iteration 19, the loss is 95.38801685732949, parameters k is -22.65763272933479 and b is 69.53972725354588\n",
      "Iteration 20, the loss is 87.46554820583378, parameters k is -21.427379361956486 and b is 69.73050328726053\n",
      "Iteration 21, the loss is 80.19236484381986, parameters k is -20.297917912315466 and b is 69.9054343836722\n",
      "Iteration 22, the loss is 73.51525485376196, parameters k is -19.260988019705383 and b is 70.06581911335984\n",
      "Iteration 23, the loss is 67.38536727965476, parameters k is -18.309006297993697 and b is 70.21284962306736\n",
      "Iteration 24, the loss is 61.75785472614873, parameters k is -17.43501085443994 and b is 70.34762035762667\n",
      "Iteration 25, the loss is 56.591545248329496, parameters k is -16.632610355452137 and b is 70.47113606707896\n",
      "Iteration 26, the loss is 51.848641131640434, parameters k is -15.895937266638883 and b is 70.58431915757562\n",
      "Iteration 27, the loss is 47.494442358176904, parameters k is -15.219604925054362 and b is 70.6880164398389\n",
      "Iteration 28, the loss is 43.497092736190105, parameters k is -14.598668129570415 and b is 70.78300532455526\n",
      "Iteration 29, the loss is 39.83599368668116, parameters k is -14.028586961048985 and b is 70.86999951002764\n",
      "Iteration 30, the loss is 36.49989349213502, parameters k is -13.505193567617946 and b is 70.94965420369853\n",
      "Iteration 31, the loss is 33.440760634493294, parameters k is -13.024661672046399 and b is 71.02257091574513\n",
      "Iteration 32, the loss is 30.64349681922099, parameters k is -12.583478578130922 and b is 71.0893018598174\n",
      "Iteration 33, the loss is 28.09680126930556, parameters k is -12.178419471287269 and b is 71.15035399311523\n",
      "Iteration 34, the loss is 25.799632422449793, parameters k is -11.806523825326858 and b is 71.20619272536291\n",
      "Iteration 35, the loss is 23.71823667835884, parameters k is -11.465073742806501 and b is 71.25724532381588\n",
      "Iteration 36, the loss is 21.839681924483468, parameters k is -11.151574070486143 and b is 71.3039040392116\n",
      "Iteration 37, the loss is 20.15506370224487, parameters k is -10.863734144416357 and b is 71.34652897553435\n",
      "Iteration 38, the loss is 18.640172964235497, parameters k is -10.599451031099925 and b is 71.3854507245896\n",
      "Iteration 39, the loss is 17.292146758858397, parameters k is -10.35679414211734 and b is 71.42097278466285\n",
      "Iteration 40, the loss is 16.097031734805036, parameters k is -10.133991109654573 and b is 71.4533737809583\n",
      "Iteration 41, the loss is 15.029744019880221, parameters k is -9.929414819596321 and b is 71.4829095040623\n",
      "Iteration 42, the loss is 14.093468558996651, parameters k is -9.741571507316939 and b is 71.50981478134544\n",
      "Iteration 43, the loss is 13.29544756122311, parameters k is -9.5690898290761 and b is 71.53430519499443\n",
      "Iteration 44, the loss is 12.60731299880895, parameters k is -9.410710829063861 and b is 71.5565786592436\n",
      "Iteration 45, the loss is 12.024680584707157, parameters k is -9.265278728692563 and b is 71.57681686834485\n",
      "Iteration 46, the loss is 11.53810452238033, parameters k is -9.131732470748647 and b is 71.59518662586986\n",
      "Iteration 47, the loss is 11.129102497712944, parameters k is -9.009097956540142 and b is 71.61184106506987\n",
      "Iteration 48, the loss is 10.7961132689998, parameters k is -8.896480919245638 and b is 71.62692076922133\n",
      "Iteration 49, the loss is 10.53103650326145, parameters k is -8.79306038132511 and b is 71.64055480015412\n",
      "Iteration 50, the loss is 10.32181741844133, parameters k is -8.698082648126036 and b is 71.65286164248711\n",
      "Iteration 51, the loss is 10.162396199675833, parameters k is -8.610855793741127 and b is 71.66395007047925\n",
      "Iteration 52, the loss is 10.045877209244997, parameters k is -8.530744598775403 and b is 71.67391994383826\n",
      "Iteration 53, the loss is 9.955497388205565, parameters k is -8.457165902986544 and b is 71.68286293830916\n",
      "Iteration 54, the loss is 9.897086180383049, parameters k is -8.389584338797757 and b is 71.69086321638765\n",
      "Iteration 55, the loss is 9.863707974560173, parameters k is -8.327508414468893 and b is 71.6979980430655\n",
      "Iteration 56, the loss is 9.84385936406296, parameters k is -8.27048691826972 and b is 71.70433835111265\n",
      "Iteration 57, the loss is 9.833647241372043, parameters k is -8.218105617347744 and b is 71.70994926003205\n",
      "Iteration 58, the loss is 9.832057743422137, parameters k is -8.169984227139 and b is 71.71489055248355\n",
      "Iteration 59, the loss is 9.837954853446048, parameters k is -8.125773629149581 and b is 71.7192171116628\n",
      "Iteration 60, the loss is 9.84904227179146, parameters k is -8.08515331675277 and b is 71.72297932283487\n",
      "Iteration 61, the loss is 9.865896269603132, parameters k is -8.04782905031485 and b is 71.72622344196037\n",
      "Iteration 62, the loss is 9.884253115120442, parameters k is -8.013530704494151 and b is 71.72899193411095\n",
      "Iteration 63, the loss is 9.904734971849663, parameters k is -7.982010291963848 and b is 71.73132378415008\n",
      "Iteration 64, the loss is 9.924840016155763, parameters k is -7.9530401490997535 and b is 71.73325478195214\n",
      "Iteration 65, the loss is 9.943642808298119, parameters k is -7.926411270359369 and b is 71.7348177842465\n",
      "Iteration 66, the loss is 9.962069070771125, parameters k is -7.9019317791662225 and b is 71.73604295500222\n",
      "Iteration 67, the loss is 9.9796504725967, parameters k is -7.879425524112272 and b is 71.73695798611215\n",
      "Iteration 68, the loss is 9.996603618439526, parameters k is -7.858730790207992 and b is 71.73758829999097\n",
      "Iteration 69, the loss is 10.012341495239292, parameters k is -7.8396991157514355 and b is 71.73795723556938\n",
      "Iteration 70, the loss is 10.027680842139432, parameters k is -7.822194206160329 and b is 71.73808621904514\n",
      "Iteration 71, the loss is 10.043368331992673, parameters k is -7.80609093682062 and b is 71.73799492064043\n",
      "Iteration 72, the loss is 10.059319657510917, parameters k is -7.791274437656179 and b is 71.73770139851204\n",
      "Iteration 73, the loss is 10.07428589271266, parameters k is -7.777639252722225 and b is 71.73722223086762\n",
      "Iteration 74, the loss is 10.0880437915863, parameters k is -7.765088568673938 and b is 71.73657263725426\n",
      "Iteration 75, the loss is 10.101612968080897, parameters k is -7.753533506465618 and b is 71.73576658990702\n",
      "Iteration 76, the loss is 10.114497737081646, parameters k is -7.7428924710983615 and b is 71.73481691597186\n",
      "Iteration 77, the loss is 10.126471301621867, parameters k is -7.7330905546589035 and b is 71.733735391351\n",
      "Iteration 78, the loss is 10.13766461327555, parameters k is -7.724058988282175 and b is 71.73253282685715\n",
      "Iteration 79, the loss is 10.14824392735104, parameters k is -7.715734639028053 and b is 71.73121914730704\n",
      "Iteration 80, the loss is 10.158080663229159, parameters k is -7.708059547991376 and b is 71.72980346413289\n",
      "Iteration 81, the loss is 10.167229401221181, parameters k is -7.700980506265966 and b is 71.72829414204297\n",
      "Iteration 82, the loss is 10.175650069248306, parameters k is -7.694448665660353 and b is 71.72669886021912\n",
      "Iteration 83, the loss is 10.183364943987023, parameters k is -7.688419181317135 and b is 71.72502466849875\n",
      "Iteration 84, the loss is 10.190431868822417, parameters k is -7.682850883621332 and b is 71.72327803895253\n",
      "Iteration 85, the loss is 10.19690394660801, parameters k is -7.677705976997354 and b is 71.72146491323505\n",
      "Iteration 86, the loss is 10.202829928174037, parameters k is -7.672949763390943 and b is 71.71959074605483\n",
      "Iteration 87, the loss is 10.208292115389117, parameters k is -7.668550388413053 and b is 71.7176605450818\n",
      "Iteration 88, the loss is 10.213376717398226, parameters k is -7.664478608288388 and b is 71.71567890758418\n",
      "Iteration 89, the loss is 10.218092516248811, parameters k is -7.660707575903602 and b is 71.71365005406277\n",
      "Iteration 90, the loss is 10.222406158624253, parameters k is -7.6572126443898165 and b is 71.71157785912881\n",
      "Iteration 91, the loss is 10.226400357108442, parameters k is -7.653971186802464 and b is 71.70946587985117\n",
      "Iteration 92, the loss is 10.23011376496926, parameters k is -7.650962430579191 and b is 71.70731738178044\n",
      "Iteration 93, the loss is 10.233529157206958, parameters k is -7.648167305564708 and b is 71.70513536284021\n",
      "Iteration 94, the loss is 10.236648942184933, parameters k is -7.64556830449069 and b is 71.70292257526029\n",
      "Iteration 95, the loss is 10.239497346656066, parameters k is -7.643149354890003 and b is 71.70068154571248\n",
      "Iteration 96, the loss is 10.242150420068507, parameters k is -7.640895701508137 and b is 71.6984145937961\n",
      "Iteration 97, the loss is 10.244608740284844, parameters k is -7.638793798351577 and b is 71.69612384900846\n",
      "Iteration 98, the loss is 10.246849894172524, parameters k is -7.636831209583291 and b is 71.69381126632469\n",
      "Iteration 99, the loss is 10.248891679889041, parameters k is -7.634996518540308 and b is 71.69147864050056\n"
     ]
    }
   ],
   "source": [
    "#initialized parameters\n",
    "def price(rm, k, b):\n",
    "    return k * rm + b\n",
    "\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "iteration_num = 100 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss2(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = partial_derivative_b(y, price_use_current_parameters)\n",
    "    \n",
    "    #update the paramters with -gradient * learning rate\n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较用MSE作为loss funciton看结果是否有很大不同：\n",
    "$$ loss = \\frac{1}{n} \\sum{(y_i - \\hat{y_i})}^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function based on MSE\n",
    "def loss(y,y_hat):\n",
    "    return sum((y_i - y_hat_i)**2 for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{loss}}{\\partial{k}} = -\\frac{2}{n}\\sum(y_i - \\hat{y_i})x_i$$\n",
    "\n",
    "$$ \\frac{\\partial{loss}}{\\partial{b}} = -\\frac{2}{n}\\sum(y_i - \\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative_k(x, y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        gradient += (y_i-y_hat_i) * x_i\n",
    "    return -2/n * gradient\n",
    "\n",
    "def partial_derivative_b(y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "        gradient += (y_i-y_hat_i)\n",
    "    return -2 / n * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 182.3151604708673, parameters k is -7.5783889011578625 and b is 71.43597614266378\n",
      "Iteration 1, the loss is 182.30848914783968, parameters k is -7.577987548029883 and b is 71.43342461000455\n",
      "Iteration 2, the loss is 182.3018181456699, parameters k is -7.5775862236831495 and b is 71.43087313569531\n",
      "Iteration 3, the loss is 182.29514746434162, parameters k is -7.577184926549193 and b is 71.4283217199811\n",
      "Iteration 4, the loss is 182.28847710383852, parameters k is -7.57678365518807 and b is 71.42577036308678\n",
      "Iteration 5, the loss is 182.28180706414378, parameters k is -7.5763824082778255 and b is 71.42321906521867\n",
      "Iteration 6, the loss is 182.2751373452416, parameters k is -7.575981184604829 and b is 71.42066782656603\n",
      "Iteration 7, the loss is 182.26846794711514, parameters k is -7.575579983054891 and b is 71.41811664730251\n",
      "Iteration 8, the loss is 182.26179886974901, parameters k is -7.575178802605118 and b is 71.4155655275874\n",
      "Iteration 9, the loss is 182.25513011312736, parameters k is -7.574777642316426 and b is 71.41301446756682\n",
      "Iteration 10, the loss is 182.24846167723402, parameters k is -7.574376501326677 and b is 71.4104634673748\n",
      "Iteration 11, the loss is 182.24179356205366, parameters k is -7.573975378844368 and b is 71.40791252713424\n",
      "Iteration 12, the loss is 182.23512576757028, parameters k is -7.573574274142845 and b is 71.40536164695787\n",
      "Iteration 13, the loss is 182.22845829376828, parameters k is -7.573173186554988 and b is 71.40281082694906\n",
      "Iteration 14, the loss is 182.22179114063178, parameters k is -7.5727721154683305 and b is 71.40026006720258\n",
      "Iteration 15, the loss is 182.21512430814545, parameters k is -7.572371060320584 and b is 71.3977093678053\n",
      "Iteration 16, the loss is 182.20845779629383, parameters k is -7.571970020595519 and b is 71.39515872883688\n",
      "Iteration 17, the loss is 182.2017916050612, parameters k is -7.571568995819199 and b is 71.39260815037028\n",
      "Iteration 18, the loss is 182.19512573443205, parameters k is -7.571167985556505 and b is 71.39005763247242\n",
      "Iteration 19, the loss is 182.18846018439072, parameters k is -7.570766989407963 and b is 71.38750717520459\n",
      "Iteration 20, the loss is 182.1817949549224, parameters k is -7.570366007006817 and b is 71.38495677862292\n",
      "Iteration 21, the loss is 182.17513004601076, parameters k is -7.56996503801635 and b is 71.38240644277884\n",
      "Iteration 22, the loss is 182.16846545764076, parameters k is -7.56956408212742 and b is 71.37985616771944\n",
      "Iteration 23, the loss is 182.16180118979688, parameters k is -7.569163139056204 and b is 71.37730595348782\n",
      "Iteration 24, the loss is 182.1551372424636, parameters k is -7.568762208542119 and b is 71.37475580012344\n",
      "Iteration 25, the loss is 182.1484736156255, parameters k is -7.568361290345919 and b is 71.3722057076624\n",
      "Iteration 26, the loss is 182.14181030926738, parameters k is -7.567960384247947 and b is 71.36965567613773\n",
      "Iteration 27, the loss is 182.13514732337347, parameters k is -7.567559490046527 and b is 71.36710570557959\n",
      "Iteration 28, the loss is 182.12848465792862, parameters k is -7.567158607556496 and b is 71.36455579601561\n",
      "Iteration 29, the loss is 182.12182231291683, parameters k is -7.566757736607843 and b is 71.36200594747099\n",
      "Iteration 30, the loss is 182.11516028832352, parameters k is -7.566356877044474 and b is 71.35945615996876\n",
      "Iteration 31, the loss is 182.1084985841327, parameters k is -7.565956028723067 and b is 71.35690643352994\n",
      "Iteration 32, the loss is 182.101837200329, parameters k is -7.565555191512028 and b is 71.35435676817372\n",
      "Iteration 33, the loss is 182.09517613689704, parameters k is -7.565154365290528 and b is 71.35180716391756\n",
      "Iteration 34, the loss is 182.08851539382152, parameters k is -7.564753549947622 and b is 71.3492576207774\n",
      "Iteration 35, the loss is 182.08185497108727, parameters k is -7.564352745381438 and b is 71.34670813876774\n",
      "Iteration 36, the loss is 182.07519486867815, parameters k is -7.563951951498434 and b is 71.34415871790179\n",
      "Iteration 37, the loss is 182.0685350865795, parameters k is -7.563551168212715 and b is 71.34160935819153\n",
      "Iteration 38, the loss is 182.0618756247754, parameters k is -7.563150395445407 and b is 71.33906005964786\n",
      "Iteration 39, the loss is 182.0552164832507, parameters k is -7.562749633124082 and b is 71.33651082228064\n",
      "Iteration 40, the loss is 182.0485576619899, parameters k is -7.562348881182227 and b is 71.33396164609883\n",
      "Iteration 41, the loss is 182.04189916097772, parameters k is -7.561948139558761 and b is 71.33141253111052\n",
      "Iteration 42, the loss is 182.0352409801987, parameters k is -7.561547408197591 and b is 71.328863477323\n",
      "Iteration 43, the loss is 182.02858311963726, parameters k is -7.561146687047203 and b is 71.32631448474287\n",
      "Iteration 44, the loss is 182.02192557927808, parameters k is -7.560745976060283 and b is 71.32376555337606\n",
      "Iteration 45, the loss is 182.01526835910602, parameters k is -7.560345275193378 and b is 71.32121668322789\n",
      "Iteration 46, the loss is 182.00861145910534, parameters k is -7.559944584406576 and b is 71.31866787430312\n",
      "Iteration 47, the loss is 182.00195487926086, parameters k is -7.559543903663218 and b is 71.316119126606\n",
      "Iteration 48, the loss is 181.9952986195568, parameters k is -7.559143232929628 and b is 71.31357044014032\n",
      "Iteration 49, the loss is 181.98864267997848, parameters k is -7.558742572174875 and b is 71.31102181490944\n",
      "Iteration 50, the loss is 181.9819870605099, parameters k is -7.55834192137054 and b is 71.30847325091631\n",
      "Iteration 51, the loss is 181.97533176113583, parameters k is -7.5579412804905175 and b is 71.3059247481635\n",
      "Iteration 52, the loss is 181.968676781841, parameters k is -7.557540649510819 and b is 71.3033763066533\n",
      "Iteration 53, the loss is 181.96202212260982, parameters k is -7.557140028409405 and b is 71.30082792638767\n",
      "Iteration 54, the loss is 181.95536778342677, parameters k is -7.556739417166024 and b is 71.29827960736826\n",
      "Iteration 55, the loss is 181.94871376427693, parameters k is -7.556338815762063 and b is 71.2957313495965\n",
      "Iteration 56, the loss is 181.94206006514472, parameters k is -7.555938224180417 and b is 71.29318315307356\n",
      "Iteration 57, the loss is 181.93540668601437, parameters k is -7.555537642405363 and b is 71.29063501780041\n",
      "Iteration 58, the loss is 181.9287536268711, parameters k is -7.5551370704224485 and b is 71.28808694377781\n",
      "Iteration 59, the loss is 181.92210088769917, parameters k is -7.554736508218387 and b is 71.28553893100634\n",
      "Iteration 60, the loss is 181.9154484684831, parameters k is -7.554335955780958 and b is 71.28299097948641\n",
      "Iteration 61, the loss is 181.90879636920792, parameters k is -7.55393541309893 and b is 71.28044308921828\n",
      "Iteration 62, the loss is 181.90214458985773, parameters k is -7.553534880161968 and b is 71.27789526020206\n",
      "Iteration 63, the loss is 181.8954931304174, parameters k is -7.553134356960566 and b is 71.27534749243773\n",
      "Iteration 64, the loss is 181.8888419908718, parameters k is -7.55273384348598 and b is 71.27279978592516\n",
      "Iteration 65, the loss is 181.8821911712049, parameters k is -7.55233333973016 and b is 71.2702521406641\n",
      "Iteration 66, the loss is 181.8755406714018, parameters k is -7.551932845685701 and b is 71.26770455665421\n",
      "Iteration 67, the loss is 181.86889049144702, parameters k is -7.5515323613457825 and b is 71.26515703389505\n",
      "Iteration 68, the loss is 181.86224063132508, parameters k is -7.551131886704126 and b is 71.26260957238611\n",
      "Iteration 69, the loss is 181.85559109102084, parameters k is -7.550731421754948 and b is 71.26006217212678\n",
      "Iteration 70, the loss is 181.84894187051873, parameters k is -7.55033096649292 and b is 71.25751483311639\n",
      "Iteration 71, the loss is 181.8422929698034, parameters k is -7.549930520913132 and b is 71.2549675553542\n",
      "Iteration 72, the loss is 181.83564438885938, parameters k is -7.549530085011055 and b is 71.2524203388394\n",
      "Iteration 73, the loss is 181.8289961276714, parameters k is -7.5491296587825145 and b is 71.24987318357115\n",
      "Iteration 74, the loss is 181.8223481862241, parameters k is -7.548729242223659 and b is 71.24732608954855\n",
      "Iteration 75, the loss is 181.8157005645022, parameters k is -7.548328835330932 and b is 71.24477905677065\n",
      "Iteration 76, the loss is 181.80905326248978, parameters k is -7.547928438101052 and b is 71.24223208523645\n",
      "Iteration 77, the loss is 181.8024062801724, parameters k is -7.547528050530985 and b is 71.23968517494492\n",
      "Iteration 78, the loss is 181.79575961753363, parameters k is -7.547127672617928 and b is 71.23713832589499\n",
      "Iteration 79, the loss is 181.7891132745587, parameters k is -7.546727304359289 and b is 71.23459153808555\n",
      "Iteration 80, the loss is 181.7824672512325, parameters k is -7.546326945752668 and b is 71.2320448115155\n",
      "Iteration 81, the loss is 181.7758215475392, parameters k is -7.545926596795844 and b is 71.22949814618363\n",
      "Iteration 82, the loss is 181.76917616346347, parameters k is -7.5455262574867605 and b is 71.2269515420888\n",
      "Iteration 83, the loss is 181.76253109898974, parameters k is -7.545125927823507 and b is 71.22440499922978\n",
      "Iteration 84, the loss is 181.75588635410335, parameters k is -7.544725607804313 and b is 71.22185851760534\n",
      "Iteration 85, the loss is 181.74924192878834, parameters k is -7.544325297427534 and b is 71.21931209721423\n",
      "Iteration 86, the loss is 181.74259782302934, parameters k is -7.54392499669164 and b is 71.21676573805519\n",
      "Iteration 87, the loss is 181.73595403681128, parameters k is -7.543524705595209 and b is 71.21421944012693\n",
      "Iteration 88, the loss is 181.7293105701188, parameters k is -7.543124424136915 and b is 71.21167320342813\n",
      "Iteration 89, the loss is 181.72266742293607, parameters k is -7.542724152315523 and b is 71.2091270279575\n",
      "Iteration 90, the loss is 181.71602459524817, parameters k is -7.54232389012988 and b is 71.20658091371372\n",
      "Iteration 91, the loss is 181.70938208703967, parameters k is -7.5419236375789085 and b is 71.20403486069543\n",
      "Iteration 92, the loss is 181.70273989829468, parameters k is -7.5415233946616 and b is 71.20148886890128\n",
      "Iteration 93, the loss is 181.69609802899862, parameters k is -7.54112316137701 and b is 71.19894293832992\n",
      "Iteration 94, the loss is 181.68945647913588, parameters k is -7.540722937724253 and b is 71.19639706897998\n",
      "Iteration 95, the loss is 181.6828152486909, parameters k is -7.540322723702494 and b is 71.19385126085007\n",
      "Iteration 96, the loss is 181.6761743376484, parameters k is -7.539922519310952 and b is 71.19130551393881\n",
      "Iteration 97, the loss is 181.6695337459931, parameters k is -7.539522324548889 and b is 71.18875982824481\n",
      "Iteration 98, the loss is 181.66289347370966, parameters k is -7.539122139415608 and b is 71.18621420376668\n",
      "Iteration 99, the loss is 181.65625352078237, parameters k is -7.538721963910451 and b is 71.183668640503\n"
     ]
    }
   ],
   "source": [
    "#initialized parameters\n",
    "\n",
    "#k = random.random() * 200 - 100  # -100 100\n",
    "#b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "iteration_num = 100 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = partial_derivative_b(y, price_use_current_parameters)\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function ： Mean absolute difference： <br>\n",
    "Iteration 99, the loss is 10.248891679889041, parameters k is -7.634996518540308 and b is 71.69147864050056\n",
    "\n",
    "Loss function : MSE <br>\n",
    "Iteration 99, the loss is 181.65625352078237, parameters k is -7.538721963910451 and b is 71.183668640503   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
