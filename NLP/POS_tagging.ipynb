{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method1: Use NLTK pos_tag() directly\n",
    "modified from: https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Social', 'NNP'),\n",
       " ('Security', 'NNP'),\n",
       " ('number', 'NN'),\n",
       " (',', ','),\n",
       " ('passport', 'JJ'),\n",
       " ('number', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('details', 'NNS'),\n",
       " ('about', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('services', 'NNS'),\n",
       " ('provided', 'VBD'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('payment', 'NN')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "text = nltk.word_tokenize(\"Social Security number , passport number and details about the services provided for the payment\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that refuse and permit both appear as a present tense verb (VBP) and a noun (NN). E.g. refUSE is a verb meaning \"deny,\" while REFuse is a noun meaning \"trash\" (i.e. they are not homophones). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world house family child country boy\n",
      "state job place way war girl work word\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "#nltk.download('brown')\n",
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made said done put had seen found given left heard was been brought\n",
      "set got that took in told felt\n"
     ]
    }
   ],
   "source": [
    "text.similar('bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time year way day country point other one man church world case\n",
      "morning moment city act state night school week\n"
     ]
    }
   ],
   "source": [
    "text.similar('summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'NN'), ('world', 'NN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(['hello', 'world'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method2: Implement pos-tagging from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id, id2tag = {}, {}  # maps tag to id . tag2id: {\"VB\": 0, \"NNP\":1,..} , id2tag: {0: \"VB\", 1: \"NNP\"....}\n",
    "word2id, id2word = {}, {} # maps word to id\n",
    "\n",
    "for line in open('traindata.txt'):\n",
    "    items = line.split('/')\n",
    "    word, tag = items[0], items[1].rstrip()  # 抽取每一行里的单词和词性\n",
    "    \n",
    "    if word not in word2id:\n",
    "        word2id[word] = len(word2id)\n",
    "        id2word[len(id2word)] = word\n",
    "    if tag not in tag2id:\n",
    "        tag2id[tag] = len(tag2id)\n",
    "        id2tag[len(id2tag)] = tag\n",
    "\n",
    "M = len(word2id)  # M: 词典的大小、# of words in dictionary\n",
    "N = len(tag2id)   # N: 词性的种类个数  # of tags in tag set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18978\n",
      "54\n",
      "{'NNP': 0, ',': 1, 'VBG': 2, 'TO': 3, 'VB': 4, 'NN': 5, 'IN': 6, 'JJ': 7, 'VBD': 8, 'NNS': 9, 'CD': 10, 'CC': 11, 'PRP': 12, 'MD': 13, 'DT': 14, '.': 15, 'VBZ': 16, 'VBN': 17, 'WDT': 18, 'VBP': 19, 'POS': 20, 'RB': 21, '$': 22, 'PRP$': 23, ':': 24, 'JJR': 25, '``': 26, \"''\": 27, 'WP': 28, 'JJS': 29, 'WRB': 30, 'RBR': 31, 'NNPS': 32, 'RP': 33, 'WP$': 34, 'EX': 35, '(': 36, ')': 37, 'PDT': 38, 'RBS': 39, 'FW': 40, 'UH': 41, 'SYM': 42, 'LS': 43, '#': 44, 'VBG|NN': 45, 'JJ|NN': 46, 'RB|IN': 47, 'NNS|NN': 48, 'VBN|JJ': 49, 'VB|NN': 50, 'RBR|JJR': 51, 'NN|NNS': 52, 'JJ|RB': 53}\n",
      "\n",
      "\n",
      "{0: 'NNP', 1: ',', 2: 'VBG', 3: 'TO', 4: 'VB', 5: 'NN', 6: 'IN', 7: 'JJ', 8: 'VBD', 9: 'NNS', 10: 'CD', 11: 'CC', 12: 'PRP', 13: 'MD', 14: 'DT', 15: '.', 16: 'VBZ', 17: 'VBN', 18: 'WDT', 19: 'VBP', 20: 'POS', 21: 'RB', 22: '$', 23: 'PRP$', 24: ':', 25: 'JJR', 26: '``', 27: \"''\", 28: 'WP', 29: 'JJS', 30: 'WRB', 31: 'RBR', 32: 'NNPS', 33: 'RP', 34: 'WP$', 35: 'EX', 36: '(', 37: ')', 38: 'PDT', 39: 'RBS', 40: 'FW', 41: 'UH', 42: 'SYM', 43: 'LS', 44: '#', 45: 'VBG|NN', 46: 'JJ|NN', 47: 'RB|IN', 48: 'NNS|NN', 49: 'VBN|JJ', 50: 'VB|NN', 51: 'RBR|JJR', 52: 'NN|NNS', 53: 'JJ|RB'}\n"
     ]
    }
   ],
   "source": [
    "print (M)\n",
    "print (N)\n",
    "print (tag2id)\n",
    "print ('\\n')\n",
    "print (id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 构建 pi, A, B\n",
    "import numpy as np\n",
    "pi = np.zeros(N)   # 每个词性出现在句子中第一个位置的概率,  N: # of tags  pi[i]: tag i出现在句子中第一个位置的概率\n",
    "A = np.zeros((N, M)) # A[i][j]: 给定tag i, 出现单词j的概率。 N: # of tags M: # of words in dictionary\n",
    "B = np.zeros((N,N))  # B[i][j]: 之前的状态是i, 之后转换成转态j的概率 N: # of tags\n",
    "print (A)\n",
    "print (B)\n",
    "print (pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_tag = \"\"\n",
    "for line in open('traindata.txt'):\n",
    "    items = line.split('/')\n",
    "    wordId, tagId = word2id[items[0]], tag2id[items[1].rstrip()]\n",
    "    if prev_tag == \"\":  # 这意味着是句子的开始\n",
    "        pi[tagId] += 1\n",
    "        A[tagId][wordId] += 1\n",
    "    else:  # 如果不是句子的开头\n",
    "        A[tagId][wordId] += 1\n",
    "        B[tag2id[prev_tag]][tagId] += 1\n",
    "    \n",
    "    if items[0] == \".\":\n",
    "        prev_tag = \"\"\n",
    "    else:\n",
    "        prev_tag = items[1].rstrip()\n",
    "\n",
    "# normalize\n",
    "pi = pi/sum(pi)\n",
    "for i in range(N):\n",
    "    A[i] /= sum(A[i])\n",
    "    B[i] /= sum(B[i])\n",
    "\n",
    "#  到此为止计算完了模型的所有的参数： pi, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(v):\n",
    "    if v == 0:\n",
    "        return np.log(v+0.000001)\n",
    "    return np.log(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x, pi, A, B):\n",
    "    \"\"\"\n",
    "    x: user input string/sentence: x: \"I like playing soccer\"\n",
    "    pi: initial probability of tags\n",
    "    A: 给定tag, 每个单词出现的概率\n",
    "    B: tag之间的转移概率\n",
    "    \"\"\"\n",
    "    x = [word2id[word] for word in x.split(\" \")]  # x: [4521, 412, 542 ..]\n",
    "    T = len(x)\n",
    "    \n",
    "    dp = np.zeros((T,N))  # dp[i][j]: w1...wi, 假设wi的tag是第j个tag\n",
    "    ptr = np.array([[0 for x in range(N)] for y in range(T)] ) # T*N\n",
    "    # TODO: ptr = np.zeros((T,N), dtype=int)\n",
    "    \n",
    "    for j in range(N): # basecase for DP算法\n",
    "        dp[0][j] = log(pi[j]) + log(A[j][x[0]])\n",
    "    \n",
    "    for i in range(1,T): # 每个单词\n",
    "        for j in range(N):  # 每个词性\n",
    "            # TODO: 以下几行代码可以写成一行（vectorize的操作， 会使得效率变高）\n",
    "            dp[i][j] = -9999999\n",
    "            for k in range(N): # 从每一个k可以到达j\n",
    "                score = dp[i-1][k] + log(B[k][j]) + log(A[j][x[i]])\n",
    "                if score > dp[i][j]:\n",
    "                    dp[i][j] = score\n",
    "                    ptr[i][j] = k\n",
    "    \n",
    "    # decoding: 把最好的tag sequence 打印出来\n",
    "    best_seq = [0]*T  # best_seq = [1,5,2,23,4,...]  \n",
    "    # step1: 找出对应于最后一个单词的词性\n",
    "    best_seq[T-1] = np.argmax(dp[T-1])\n",
    "    \n",
    "    # step2: 通过从后到前的循环来依次求出每个单词的词性\n",
    "    for i in range(T-2, -1, -1): # T-2, T-1,... 1, 0\n",
    "        best_seq[i] = ptr[i+1][best_seq[i+1]]\n",
    "        \n",
    "    # 到目前为止, best_seq存放了对应于x的 词性序列\n",
    "    for i in range(len(best_seq)):\n",
    "        print (id2tag[best_seq[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP\n",
      "NNP\n",
      "NN\n",
      ",\n",
      "NN\n",
      "NN\n",
      "CC\n",
      "NNS\n",
      "IN\n",
      "DT\n",
      "NNS\n",
      "VBN\n",
      "IN\n",
      "DT\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "x = \"Social Security number , passport number and details about the services provided for the payment\"\n",
    "viterbi(x, pi, A, B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
